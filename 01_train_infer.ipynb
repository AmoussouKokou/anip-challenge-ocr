{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OCR + Détection de fraude sur documents d'identité\n",
        "\n",
        "Pipeline complet : prétraitement (alignement), OCR multilingue, classification image, features OCR tabulaires, *late fusion*, inférence test (JSON champs + CSV classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3692a290",
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c782330",
      "metadata": {},
      "source": [
        "## 0) Installation / utils\n",
        "Exécuter ce bloc si nécessaire dans votre environnement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1a8c9d35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install torch torchvision torchaudio pytorch-lightning==2.4.0 timm==1.0.9 albumentations opencv-python-headless shapely rapidfuzz python-Levenshtein paddlepaddle-gpu paddleocr lightgbm scikit-learn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cb550074",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2876240f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5ab4eb1",
      "metadata": {},
      "source": [
        "utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "43fb60e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, random, numpy as np, torch\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    import random, numpy as np, torch\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def find_images(root, exts=(\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\")):\n",
        "    out=[]\n",
        "    for dp,_,files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower().endswith(exts):\n",
        "                out.append(os.path.join(dp,f))\n",
        "    return out\n",
        "\n",
        "def save_json(obj, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as w:\n",
        "        json.dump(obj, w, ensure_ascii=False, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e8e291",
      "metadata": {},
      "source": [
        "constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "df11f9e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "CLASSES = [\"normal\",\"forgery_1\",\"forgery_2\",\"forgery_3\",\"forgery_4\"]\n",
        "COUNTRIES = [\"spain\",\"estonia\",\"russia\",\"arizona\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a7a94a3",
      "metadata": {},
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "13945737",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Projet\\anip-challenge-ocr\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, glob, random, cv2, numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "# from .constants import CLASSES\n",
        "\n",
        "# def make_transforms(train=True, size=768):\n",
        "#     aug = [A.LongestMaxSize(size), A.PadIfNeeded(size,size, border_mode=cv2.BORDER_CONSTANT, value=(255,255,255))]\n",
        "#     if train:\n",
        "#         aug += [\n",
        "#             A.ImageCompression(quality_lower=40,quality_upper=90,p=0.5),\n",
        "#             A.MotionBlur(3,p=0.2), A.GaussianBlur(3,p=0.2),\n",
        "#             A.RandomBrightnessContrast(0.2,0.2,p=0.5),\n",
        "#             A.Rotate(limit=7, border_mode=cv2.BORDER_CONSTANT, value=(255,255,255),p=0.5),\n",
        "#         ]\n",
        "#     aug += [A.Normalize(), ToTensorV2()]\n",
        "#     return A.Compose(aug)\n",
        "\n",
        "\n",
        "import inspect\n",
        "import albumentations as A\n",
        "import cv2\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "def _init_with_supported_kwargs(cls, **kwargs):\n",
        "    \"\"\"Construit un transform Albumentations en ne gardant que les kwargs supportés.\"\"\"\n",
        "    sig = inspect.signature(cls.__init__)\n",
        "    allowed = set(sig.parameters.keys())\n",
        "    # __init__(self, ...) -> enlever 'self'\n",
        "    allowed.discard('self')\n",
        "    filtered = {k: v for k, v in kwargs.items() if k in allowed}\n",
        "    return cls(**filtered)\n",
        "\n",
        "def _pad_if_needed(size):\n",
        "    # Essaie d’abord border_value (nouvelles versions), sinon value (anciennes).\n",
        "    # On filtre automatiquement selon la signature présente en 2.0.8.\n",
        "    return _init_with_supported_kwargs(\n",
        "        A.PadIfNeeded,\n",
        "        min_height=size, min_width=size,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        border_value=(255, 255, 255),\n",
        "        value=(255, 255, 255),         # au cas où ta build attend 'value'\n",
        "        position='center'\n",
        "    )\n",
        "\n",
        "def _rotate(limit):\n",
        "    return _init_with_supported_kwargs(\n",
        "        A.Rotate,\n",
        "        limit=limit,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        border_value=(255, 255, 255),\n",
        "        value=(255, 255, 255),\n",
        "        p=0.5\n",
        "    )\n",
        "\n",
        "def _image_compression():\n",
        "    return _init_with_supported_kwargs(\n",
        "        A.ImageCompression,\n",
        "        quality_range=(40, 90),\n",
        "        quality_lower=40, quality_upper=90,\n",
        "        p=0.5\n",
        "    )\n",
        "\n",
        "def make_transforms(train=True, size=768):\n",
        "    aug = [\n",
        "        _init_with_supported_kwargs(A.LongestMaxSize, max_size=size, p=1.0),\n",
        "        _pad_if_needed(size),\n",
        "    ]\n",
        "    if train:\n",
        "        aug += [\n",
        "            _image_compression(),\n",
        "            _init_with_supported_kwargs(A.MotionBlur, blur_limit=3, p=0.2),\n",
        "            _init_with_supported_kwargs(A.GaussianBlur, blur_limit=3, p=0.2),\n",
        "            _init_with_supported_kwargs(A.RandomBrightnessContrast,\n",
        "                                        brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            _rotate(limit=7),\n",
        "        ]\n",
        "    aug += [A.Normalize(), ToTensorV2()]\n",
        "    return A.Compose(aug)\n",
        "\n",
        "\n",
        "\n",
        "def infer_country_from_path(p):\n",
        "    p=p.lower()\n",
        "    if \"estonia\" in p or \"ee\" in p: return \"ee\"\n",
        "    if \"spain\" in p or \"es\" in p: return \"es\"\n",
        "    if \"russia\" in p or \"ru\" in p: return \"ru\"\n",
        "    if \"arizona\" in p or \"az\" in p or \"usa\" in p: return \"az\"\n",
        "    return \"unknown\"\n",
        "\n",
        "class IdDocsDataset(Dataset):\n",
        "    def __init__(self, root, train=True, size=768):\n",
        "        self.root = root\n",
        "        self.items=[]\n",
        "        for country in os.listdir(root):\n",
        "            cdir = os.path.join(root, country)\n",
        "            if not os.path.isdir(cdir): continue\n",
        "            for cls in CLASSES:\n",
        "                img_dir = os.path.join(cdir, cls if cls!=\"normal\" else \"normal\")\n",
        "                if not os.path.isdir(img_dir): continue\n",
        "                for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.tif\",\"*.tiff\"):\n",
        "                    for imgp in glob.glob(os.path.join(img_dir,ext)):\n",
        "                        self.items.append({\"img\": imgp, \"label\": cls, \"country\": infer_country_from_path(imgp)})\n",
        "        random.shuffle(self.items)\n",
        "        self.transforms = make_transforms(train=train, size=size)\n",
        "        self.class_to_idx = {c:i for i,c in enumerate(CLASSES)}\n",
        "\n",
        "    def __len__(self): return len(self.items)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        x = self.items[i]\n",
        "        img = cv2.imread(x[\"img\"]); \n",
        "        if img is None:\n",
        "            raise FileNotFoundError(f\"Impossible de lire: {x['img']}\")\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        t = self.transforms(image=img)[\"image\"]\n",
        "        y = self.class_to_idx[x[\"label\"]]\n",
        "        return {\"image\": t, \"label\": y, \"country\": x[\"country\"], \"path\": x[\"img\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29f24204",
      "metadata": {},
      "source": [
        "align"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "105f0c53",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import cv2, numpy as np\n",
        "\n",
        "def compute_homography(img, template):\n",
        "    orb = cv2.ORB_create(nfeatures=3000)\n",
        "    kp1, des1 = orb.detectAndCompute(img,None)\n",
        "    kp2, des2 = orb.detectAndCompute(template,None)\n",
        "    if des1 is None or des2 is None: return None\n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
        "    matches = bf.knnMatch(des1, des2, k=2)\n",
        "    good=[]\n",
        "    for m,n in matches:\n",
        "        if m.distance < 0.75*n.distance:\n",
        "            good.append(m)\n",
        "    if len(good)<12: return None\n",
        "    src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
        "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
        "    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
        "    return H\n",
        "\n",
        "def warp_to_template(img, template):\n",
        "    H = compute_homography(img, template)\n",
        "    if H is None: \n",
        "        return img, None\n",
        "    h,w = template.shape[:2]\n",
        "    warped = cv2.warpPerspective(img, H, (w,h), borderValue=(255,255,255))\n",
        "    return warped, H\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b2da49",
      "metadata": {},
      "source": [
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dd5792f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np, re\n",
        "\n",
        "def iou(boxA, boxB):\n",
        "    xA=max(boxA[0], boxB[0]); yA=max(boxA[1], boxB[1])\n",
        "    xB=min(boxA[0]+boxA[2], boxB[0]+boxB[2]); yB=min(boxA[1]+boxA[3], boxB[1]+boxB[3])\n",
        "    inter=max(0,xB-xA)*max(0,yB-yA)\n",
        "    union=boxA[2]*boxA[3]+boxB[2]*boxB[3]-inter\n",
        "    return inter/union if union>0 else 0.0\n",
        "\n",
        "def mrz_checksum(s):\n",
        "    weights=[7,3,1]; total=0\n",
        "    def val(c):\n",
        "        if c.isdigit(): return int(c)\n",
        "        if 'A'<=c<='Z': return ord(c)-55\n",
        "        return 0\n",
        "    for i,c in enumerate(s):\n",
        "        total += val(c) * weights[i%3]\n",
        "    return str(total % 10)\n",
        "\n",
        "def build_features(ocr_items, expected_fields):\n",
        "    feats={}\n",
        "    ious=[]; confs=[]; miss=0; regex_ok=0; lens=[]\n",
        "    for f in expected_fields:\n",
        "        eb=f[\"bbox\"]\n",
        "        best=None; best_d=1e18\n",
        "        ex=(eb[0]+eb[2]/2, eb[1]+eb[3]/2)\n",
        "        for it in ocr_items:\n",
        "            xs=[p[0] for p in it[\"box\"]]; ys=[p[1] for p in it[\"box\"]]\n",
        "            cx,cy=sum(xs)/4,sum(ys)/4\n",
        "            d=(cx-ex[0])**2+(cy-ex[1])**2\n",
        "            if d<best_d: best=it; best_d=d\n",
        "        if best is None: miss+=1; continue\n",
        "        xs=[p[0] for p in best[\"box\"]]; ys=[p[1] for p in best[\"box\"]]\n",
        "        bb=[min(xs),min(ys), max(xs)-min(xs), max(ys)-min(ys)]\n",
        "        ious.append(iou(eb,bb))\n",
        "        confs.append(best[\"conf\"])\n",
        "        txt=best[\"text\"]\n",
        "        lens.append(min(len(txt), 64))\n",
        "    feats[\"iou_mean\"]=float(np.mean(ious)) if ious else 0.0\n",
        "    feats[\"conf_mean\"]=float(np.mean(confs)) if confs else 0.0\n",
        "    feats[\"missing_ratio\"]=float(miss/max(1,len(expected_fields)))\n",
        "    feats[\"text_len_mean\"]=float(np.mean(lens)) if lens else 0.0\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf6a3640",
      "metadata": {},
      "source": [
        "fuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "218a84ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def fuse_probs(p_img, p_tab, alpha=0.6):\n",
        "    return alpha*np.array(p_img)+(1-alpha)*np.array(p_tab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f77b986",
      "metadata": {},
      "source": [
        "model_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4dd19c4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, timm, pytorch_lightning as pl\n",
        "import torch.nn as nn\n",
        "import torchmetrics\n",
        "\n",
        "NUM_CLASSES=5\n",
        "\n",
        "class ImgClassifier(pl.LightningModule):\n",
        "    def __init__(self, lr=1e-4, wd=1e-4, class_weights=None, model_name=\"tf_efficientnet_b0\"):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = timm.create_model(model_name, pretrained=True, num_classes=NUM_CLASSES)\n",
        "        self.crit = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        self.f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\")\n",
        "\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "    def step(self, batch, stage):\n",
        "        y = batch[\"label\"]; yhat = self.forward(batch[\"image\"])\n",
        "        loss = self.crit(yhat, y)\n",
        "        preds = yhat.argmax(1)\n",
        "        f1 = self.f1(preds, y)\n",
        "        self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
        "        self.log(f\"{stage}_f1\", f1, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self,batch,_): return self.step(batch,\"train\")\n",
        "    def validation_step(self,batch,_): return self.step(batch,\"val\")\n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.wd)\n",
        "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
        "        return {\"optimizer\":opt,\"lr_scheduler\":sch}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b015fab5",
      "metadata": {},
      "source": [
        "model_tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "29471ed4",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch, torch.nn as nn, pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "\n",
        "class TabClassifier(pl.LightningModule):\n",
        "    def __init__(self, in_dim, lr=1e-3, wd=1e-4, hidden=128):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.m = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden), nn.ReLU(), nn.BatchNorm1d(hidden),\n",
        "            nn.Linear(hidden, hidden), nn.ReLU(), nn.BatchNorm1d(hidden),\n",
        "            nn.Linear(hidden, 5)\n",
        "        )\n",
        "        self.crit = nn.CrossEntropyLoss()\n",
        "        self.f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=5, average=\"macro\")\n",
        "\n",
        "    def forward(self,x): return self.m(x)\n",
        "    def step(self,b,stage):\n",
        "        y=b[\"y\"]; yhat=self.forward(b[\"x\"]); loss=self.crit(yhat,y)\n",
        "        preds=yhat.argmax(1); f1=self.f1(preds,y)\n",
        "        self.log(f\"{stage}_loss\",loss,prog_bar=True); self.log(f\"{stage}_f1\",f1,prog_bar=True)\n",
        "        return loss\n",
        "    def training_step(self,b,_): return self.step(b,\"train\")\n",
        "    def validation_step(self,b,_): return self.step(b,\"val\")\n",
        "    def configure_optimizers(self):\n",
        "        opt=torch.optim.AdamW(self.parameters(),lr=self.hparams.lr,weight_decay=self.hparams.wd)\n",
        "        return opt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "662851dd",
      "metadata": {},
      "source": [
        "ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "51897a95",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install paddlepaddle-gpu==2.6.1 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "22aa1175",
      "metadata": {},
      "outputs": [],
      "source": [
        "from paddleocr import PaddleOCR\n",
        "\n",
        "LANGS_BY_COUNTRY = {\"es\":[\"es\",\"en\"], \"ee\":[\"en\"], \"ru\":[\"ru\",\"en\"], \"az\":[\"en\"]}\n",
        "\n",
        "class OCRWrapper:\n",
        "    def __init__(self, country=\"es\"):\n",
        "        self.country = country\n",
        "        # Note: 'multilang' suppose que les modèles multilingues sont installés\n",
        "        self.ocr = PaddleOCR(use_angle_cls=True, lang='multilang', show_log=False)\n",
        "\n",
        "    def run(self, img):\n",
        "        res = self.ocr.ocr(img, cls=True)\n",
        "        out=[]\n",
        "        if res and len(res)>0:\n",
        "            for line in res[0]:\n",
        "                box = line[0]; text=line[1][0]; conf=line[1][1]\n",
        "                out.append({\"box\":box, \"text\":text, \"conf\":float(conf)})\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34deb5c",
      "metadata": {},
      "source": [
        "schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bdc6d4dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os, json, numpy as np\n",
        "\n",
        "# class SchemaManager:\n",
        "#     def __init__(self):\n",
        "#         self.templates = {}  # {country: {\"template_image\": ndarray or None, \"fields\":[{\"key\":..., \"bbox\":[x,y,w,h], \"regex\":..., \"lang\":...}] }}\n",
        "\n",
        "#     def load_country_schema(self, country, gt_dir):\n",
        "#         # Agrège les bboxes de gt/*.json -> bbox médiane par clé\n",
        "#         boxes_by_key = {}\n",
        "#         for fn in os.listdir(gt_dir):\n",
        "#             if not fn.lower().endswith(\".json\"): continue\n",
        "#             with open(os.path.join(gt_dir, fn), \"r\", encoding=\"utf-8\") as r:\n",
        "#                 obj = json.load(r)\n",
        "#             for k,v in obj.items():\n",
        "#                 bb = v.get(\"bbox\") or v.get(\"box\") or v.get(\"bbox_xywh\")\n",
        "#                 if bb is None: \n",
        "#                     continue\n",
        "#                 boxes_by_key.setdefault(k, []).append(bb)\n",
        "#         fields=[]\n",
        "#         for k, arr in boxes_by_key.items():\n",
        "#             arr_np = np.array(arr, dtype=float)\n",
        "#             med = np.median(arr_np, axis=0).tolist()\n",
        "#             fields.append({\"key\":k, \"bbox\": [float(med[0]), float(med[1]), float(med[2]), float(med[3])] })\n",
        "#         self.templates[country] = {\"template_image\": None, \"fields\": fields}\n",
        "\n",
        "#     def set_template_image(self, country, img):\n",
        "#         self.templates.setdefault(country, {\"template_image\": None, \"fields\": []})\n",
        "#         self.templates[country][\"template_image\"] = img\n",
        "\n",
        "#     def fields_for_country(self, country):\n",
        "#         return self.templates.get(country, {}).get(\"fields\", [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "133eee3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json\n",
        "\n",
        "class SchemaManager:\n",
        "    \"\"\"\n",
        "    Schéma basé sur les CLÉS seulement (pas de bboxes).\n",
        "    - fields = [{\"key\": <nom_du_champ>}]\n",
        "    - has_bbox = False\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.templates = {}  # {country: {\"template_image\": None, \"fields\":[{\"key\":...}], \"has_bbox\": False}}\n",
        "\n",
        "    def load_country_schema(self, country, gt_dir):\n",
        "        keys_counter = {}\n",
        "        n_files = 0\n",
        "        for fn in os.listdir(gt_dir):\n",
        "            if not fn.lower().endswith(\".json\"):\n",
        "                continue\n",
        "            n_files += 1\n",
        "            path = os.path.join(gt_dir, fn)\n",
        "            try:\n",
        "                with open(path, \"r\", encoding=\"utf-8\") as r:\n",
        "                    obj = json.load(r)\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] JSON invalide ignoré: {path} ({e})\")\n",
        "                continue\n",
        "\n",
        "            # On attend un JSON plat {key: value}; on collecte juste les clés\n",
        "            if isinstance(obj, dict):\n",
        "                for k in obj.keys():\n",
        "                    keys_counter[k] = keys_counter.get(k, 0) + 1\n",
        "\n",
        "        # Construit la liste ordonnée des clés les plus fréquentes\n",
        "        fields = [{\"key\": k} for k, _ in sorted(keys_counter.items(), key=lambda kv: (-kv[1], kv[0]))]\n",
        "        self.templates[country] = {\n",
        "            \"template_image\": None,\n",
        "            \"fields\": fields,\n",
        "            \"has_bbox\": False\n",
        "        }\n",
        "        print(f\"[{country}] gt lus: {n_files}, clés uniques: {len(fields)}, has_bbox=False\")\n",
        "\n",
        "    def set_template_image(self, country, img):\n",
        "        self.templates.setdefault(country, {\"template_image\": None, \"fields\": [], \"has_bbox\": False})\n",
        "        self.templates[country][\"template_image\"] = img\n",
        "\n",
        "    def fields_for_country(self, country):\n",
        "        return self.templates.get(country, {}).get(\"fields\", [])\n",
        "\n",
        "    def has_bbox(self, country):\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51901be3",
      "metadata": {},
      "source": [
        "## 0) helpers OCR pour chaque clé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7f4fe5bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "DATE_RE = re.compile(r\"\\b(\\d{2})[ ./-](\\d{2})[ ./-](\\d{2,4})\\b\")\n",
        "DNI_RE  = re.compile(r\"\\b(\\d{8})([A-Z])\\b\")\n",
        "ALNUM_RE = re.compile(r\"[A-Z0-9]{6,}\")\n",
        "\n",
        "# Calcul de la lettre de contrôle du DNI (Espagne)\n",
        "_DNI_LETTERS = \"TRWAGMYFPDXBNJZSQVHLCKE\"\n",
        "def dni_letter(num_8digits: str) -> str:\n",
        "    try:\n",
        "        n = int(num_8digits)\n",
        "        return _DNI_LETTERS[n % 23]\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def _norm_date(d, prefer_format=\"DD/MM/YYYY\"):\n",
        "    m = DATE_RE.search(d.replace(\"\\\\\", \"/\"))\n",
        "    if not m:\n",
        "        return \"\"\n",
        "    dd, mm, yy = m.groups()\n",
        "    if len(yy) == 2:\n",
        "        yy = \"20\"+yy if int(yy) <= 30 else \"19\"+yy\n",
        "    try:\n",
        "        dt = datetime(int(yy), int(mm), int(dd))\n",
        "        if prefer_format == \"YYYY-MM-DD\":\n",
        "            return dt.strftime(\"%Y-%m-%d\")\n",
        "        return dt.strftime(\"%d/%m/%Y\")\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def pick_text_for_key_from_ocr(key: str, ocr_items: list):\n",
        "    k = key.lower()\n",
        "\n",
        "    # country_code : 3 lettres (ESP, EST, RUS, USA/AZ…)\n",
        "    if \"country_code\" in k:\n",
        "        # chercher tokens de 3 lettres\n",
        "        best = \"\"\n",
        "        best_conf = -1\n",
        "        for it in ocr_items:\n",
        "            for tok in re.findall(r\"\\b[A-Z]{3}\\b\", it[\"text\"].upper()):\n",
        "                if it[\"conf\"] > best_conf:\n",
        "                    best, best_conf = tok, it[\"conf\"]\n",
        "        return best or \"ESP\"  # défaut si rien\n",
        "\n",
        "    # dates\n",
        "    if any(s in k for s in [\"birthday\", \"issue_date\", \"expire_date\", \"fecha\", \"expiry\", \"expiration\"]):\n",
        "        for it in ocr_items:\n",
        "            s = _norm_date(it[\"text\"])\n",
        "            if s:\n",
        "                return s\n",
        "        return \"\"\n",
        "\n",
        "    # genre\n",
        "    if \"gender\" in k or \"sex\" in k:\n",
        "        for it in ocr_items:\n",
        "            m = re.search(r\"\\b([MF])\\b\", it[\"text\"].upper())\n",
        "            if m:\n",
        "                return m.group(1)\n",
        "        return \"\"\n",
        "\n",
        "    # DNI / card_num : 8 chiffres + 1 lettre\n",
        "    if \"card_num\" in k or (\"dni\" in k) or (\"document\" in k and \"num\" in k):\n",
        "        best = \"\"\n",
        "        best_conf = -1\n",
        "        for it in ocr_items:\n",
        "            for m in DNI_RE.finditer(it[\"text\"].replace(\" \", \"\").upper()):\n",
        "                num, letter = m.groups()\n",
        "                if dni_letter(num) == letter and it[\"conf\"] > best_conf:\n",
        "                    best, best_conf = num + letter, it[\"conf\"]\n",
        "        if best:\n",
        "            return best\n",
        "        # fallback : meilleur alphanum\n",
        "        for it in ocr_items:\n",
        "            m = ALNUM_RE.search(it[\"text\"].replace(\" \", \"\").upper())\n",
        "            if m and it[\"conf\"] > best_conf:\n",
        "                best, best_conf = m.group(0), it[\"conf\"]\n",
        "        return best\n",
        "\n",
        "    # personal_num : identifiant alphanumérique\n",
        "    if \"personal_num\" in k or \"personal\" in k:\n",
        "        best = \"\"\n",
        "        best_conf = -1\n",
        "        for it in ocr_items:\n",
        "            m = ALNUM_RE.search(it[\"text\"].replace(\" \", \"\").upper())\n",
        "            if m and it[\"conf\"] > best_conf:\n",
        "                best, best_conf = m.group(0), it[\"conf\"]\n",
        "        return best\n",
        "\n",
        "    # noms/prénoms : on prend la ligne la plus confiante en majuscules\n",
        "    if any(s in k for s in [\"surname\", \"given_name\", \"second_surname\", \"name\"]):\n",
        "        best = \"\"\n",
        "        best_conf = -1\n",
        "        for it in ocr_items:\n",
        "            t = it[\"text\"].strip()\n",
        "            if t.upper() == t and len(t) >= 2 and it[\"conf\"] > best_conf:\n",
        "                best, best_conf = t, it[\"conf\"]\n",
        "        if best:\n",
        "            return best\n",
        "\n",
        "    # fallback général : meilleure confiance\n",
        "    if ocr_items:\n",
        "        it = max(ocr_items, key=lambda x: x.get(\"conf\", 0.0))\n",
        "        return it[\"text\"]\n",
        "    return \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c9e340a",
      "metadata": {},
      "source": [
        "## 1) Imports & config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0d5d533c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, cv2, numpy as np, pandas as pd, torch\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "# from src.utils import seed_everything, save_json\n",
        "# from src.data import IdDocsDataset, infer_country_from_path, make_transforms\n",
        "# from src.align import warp_to_template\n",
        "# from src.ocr import OCRWrapper\n",
        "# from src.schema import SchemaManager\n",
        "# from src.features import build_features\n",
        "# from src.model_img import ImgClassifier\n",
        "# from src.model_tab import TabClassifier\n",
        "# from src.fuse import fuse_probs\n",
        "# from src.constants import CLASSES\n",
        "\n",
        "seed_everything(42)\n",
        "DATA_ROOT = \"data\" \n",
        "TRAIN_ROOT = os.path.join(DATA_ROOT, \"train\")\n",
        "TEST_ROOT  = os.path.join(DATA_ROOT, \"test\")\n",
        "WEIGHTS_DIR = \"../weights\"\n",
        "SUB_DIR = \"../submissions\"\n",
        "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(SUB_DIR, \"fields_json\"), exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Lecture d'images (aperçu rapide)\n",
        "Ici on recense les images d'entraînement et on affiche quelques exemples (optionnel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_all = IdDocsDataset(TRAIN_ROOT, train=True, size=768)\n",
        "# print(f\"Total images train: {len(ds_all)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7f657490",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'image': tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          ...,\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
              " \n",
              "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          ...,\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
              " \n",
              "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          ...,\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
              " 'label': 2,\n",
              " 'country': 'es',\n",
              " 'path': 'data\\\\train\\\\esp\\\\forgery_2\\\\generated.photos_v3_0044110.png'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Exemple item\n",
        "ds_all[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ecff866e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'normal': 0, 'forgery_1': 1, 'forgery_2': 2, 'forgery_3': 3, 'forgery_4': 4}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_all.class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b70dbe16",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'img': 'data\\\\train\\\\esp\\\\forgery_2\\\\generated.photos_v3_0044110.png',\n",
              " 'label': 'forgery_2',\n",
              " 'country': 'es'}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_all.items[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Gabarits & schémas depuis gt/\n",
        "On construit un schéma par pays (bbox médiane par champ) à partir de `train/<country>/gt/*.json`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[arizona_dl] gt lus: 500, clés uniques: 12, has_bbox=False\n",
            "[esp] gt lus: 500, clés uniques: 10, has_bbox=False\n",
            "[est] gt lus: 500, clés uniques: 13, has_bbox=False\n",
            "[rus] gt lus: 500, clés uniques: 10, has_bbox=False\n",
            "Pays chargés: ['arizona_dl', 'esp', 'est', 'rus']\n"
          ]
        }
      ],
      "source": [
        "schema = SchemaManager()\n",
        "for country_dir in os.listdir(TRAIN_ROOT):\n",
        "    gt_dir = os.path.join(TRAIN_ROOT, country_dir, \"gt\")\n",
        "    if os.path.isdir(gt_dir):\n",
        "        schema.load_country_schema(country_dir, gt_dir)\n",
        "print(\"Pays chargés:\", list(schema.templates.keys()))\n",
        "# Optionnel: charger des templates images par pays si disponibles (ex: une image normale de référence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) DataLoaders PyTorch (train/val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA dispo: True\n",
            "torch: 2.5.1+cu121\n",
            "torch.version.cuda: 12.1\n",
            "GPU: NVIDIA GeForce RTX 2050\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(10000, 8500, 1500)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds = ds_all\n",
        "n = len(ds); n_val = max(1, int(0.15*n)); n_train = max(1, n-n_val)\n",
        "train_ds, val_ds = random_split(ds, [n_train, n_val])\n",
        "# train_dl = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
        "# val_dl   = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "import torch\n",
        "print(\"CUDA dispo:\", torch.cuda.is_available())\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"torch.version.cuda:\", torch.version.cuda)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    \n",
        "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True,\n",
        "                      num_workers=4 if use_cuda else 0,\n",
        "                      pin_memory=use_cuda, persistent_workers=use_cuda)\n",
        "val_dl   = DataLoader(val_ds, batch_size=16, shuffle=False,\n",
        "                      num_workers=4 if use_cuda else 0,\n",
        "                      pin_memory=use_cuda, persistent_workers=use_cuda)\n",
        "n, n_train, n_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Entraînement modèle image (Lightning + timm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "312b7e08",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, os\n",
        "torch.set_float32_matmul_precision(\"high\")     # kernels matmul plus rapides\n",
        "torch.backends.cudnn.benchmark = True          # profils optimisés si tailles fixes\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"       # au cas où (1 seul GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd9afbc8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "d:\\Projet\\anip-challenge-ocr\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name | Type              | Params | Mode \n",
            "---------------------------------------------------\n",
            "0 | net  | EfficientNet      | 4.0 M  | train\n",
            "1 | crit | CrossEntropyLoss  | 0      | train\n",
            "2 | f1   | MulticlassF1Score | 0      | train\n",
            "---------------------------------------------------\n",
            "4.0 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.0 M     Total params\n",
            "16.056    Total estimated model params size (MB)\n",
            "339       Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "# ==== Modèle ====\n",
        "# (Optionnel) poids de classes si dataset déséquilibré :\n",
        "# class_weights = torch.tensor([1.,1.,1.,1.,1.], dtype=torch.float32, device=\"cuda\")\n",
        "class_weights = None\n",
        "\n",
        "model_img = ImgClassifier(\n",
        "    lr=1e-4, wd=1e-4,\n",
        "    class_weights=class_weights,\n",
        "    model_name=\"tf_efficientnet_b0\"\n",
        ")\n",
        "# petit gain mémoire/perf CNN\n",
        "model_img = model_img.to(memory_format=torch.channels_last)\n",
        "\n",
        "# ==== Callbacks & Trainer ====\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "class PrintMetricsCallback(Callback):\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        # récupère les derniers logs connus\n",
        "        logs = trainer.callback_metrics\n",
        "        f1 = logs.get(\"val_f1\")\n",
        "        loss = logs.get(\"val_loss\")\n",
        "        epoch = trainer.current_epoch\n",
        "        if f1 is not None and loss is not None:\n",
        "            print(f\"\\n[Epoch {epoch}] val_loss={loss:.4f} | val_f1={f1:.4f}\")\n",
        "\n",
        "ckpt = ModelCheckpoint(\n",
        "    monitor=\"val_f1\", mode=\"max\", save_top_k=1,\n",
        "    dirpath=WEIGHTS_DIR, filename=\"best_img\"\n",
        ")\n",
        "es = EarlyStopping(monitor=\"val_f1\", mode=\"max\", patience=5)\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=12,                 # un peu plus que 10 pour le CosineAnnealingLR du modèle\n",
        "    callbacks=[ckpt, es],\n",
        "    accelerator=\"gpu\", devices=1,  # force l'usage GPU (évite le fallback CPU silencieux)\n",
        "    precision=\"16-mixed\",          # AMP\n",
        "    gradient_clip_val=1.0,         # stable sur lots un peu gros\n",
        "    accumulate_grad_batches=1,     # ↑ si tu veux un batch effectif plus grand\n",
        "    log_every_n_steps=20\n",
        ")\n",
        "\n",
        "trainer.fit(model_img, train_dl, val_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "# class_weights=None  # éventuellement torch.tensor([...], device='cuda')\n",
        "# model_img = ImgClassifier(lr=1e-4, wd=1e-4, class_weights=class_weights, model_name=\"tf_efficientnet_b0\")\n",
        "# ckpt = ModelCheckpoint(monitor=\"val_f1\", mode=\"max\", save_top_k=1, dirpath=WEIGHTS_DIR, filename=\"best_img\")\n",
        "# es = EarlyStopping(monitor=\"val_f1\", mode=\"max\", patience=5)\n",
        "# trainer = pl.Trainer(max_epochs=10, callbacks=[ckpt,es], accelerator=\"auto\", devices=\"auto\", precision=\"16-mixed\")\n",
        "# trainer.fit(model_img, train_dl, val_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Construction features OCR sur l'ensemble de validation\n",
        "On aligne (si template dispo), on applique PaddleOCR, on agrège des features par image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "rows=[]\n",
        "for b in val_dl:\n",
        "    ims=b[\"image\"]; ys=b[\"label\"]; countries=b[\"country\"]; paths=b[\"path\"]\n",
        "    for img_t, y, country, p in zip(ims, ys, countries, paths):\n",
        "        img = (img_t.permute(1,2,0).numpy()*255).astype(np.uint8)\n",
        "        template = None\n",
        "        if country in schema.templates and schema.templates[country][\"template_image\"] is not None:\n",
        "            warped,_ = warp_to_template(img, schema.templates[country][\"template_image\"])\n",
        "        else:\n",
        "            warped = img\n",
        "        ocr = OCRWrapper(country).run(warped)\n",
        "        expected = schema.fields_for_country(country)\n",
        "        feats = build_features(ocr, expected)\n",
        "        feats[\"y\"]=int(y); feats[\"image_id\"]=os.path.basename(p)\n",
        "        rows.append(feats)\n",
        "import pandas as pd\n",
        "df_feats = pd.DataFrame(rows).fillna(0.0)\n",
        "feat_cols = [c for c in df_feats.columns if c not in [\"y\",\"image_id\"]]\n",
        "df_feats.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Modèle tabulaire (MLP) sur features OCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from src.model_tab import TabClassifier\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "X = torch.tensor(df_feats[feat_cols].values, dtype=torch.float32)\n",
        "Y = torch.tensor(df_feats[\"y\"].values, dtype=torch.long)\n",
        "ds_tab = TensorDataset(X,Y)\n",
        "n=len(ds_tab); n_val2=max(1,int(0.2*n)); n_tr=n-n_val2\n",
        "train_tab, val_tab = random_split(ds_tab,[n_tr,n_val2])\n",
        "train_tab_dl=DataLoader(train_tab,batch_size=64,shuffle=True)\n",
        "val_tab_dl=DataLoader(val_tab,batch_size=64,shuffle=False)\n",
        "\n",
        "tab = TabClassifier(in_dim=len(feat_cols), lr=1e-3, wd=1e-4, hidden=128)\n",
        "ckpt2 = pl.callbacks.ModelCheckpoint(monitor=\"val_f1\", mode=\"max\", save_top_k=1, dirpath=WEIGHTS_DIR, filename=\"best_tab\")\n",
        "trainer2 = pl.Trainer(max_epochs=20, callbacks=[ckpt2], accelerator=\"auto\", devices=\"auto\", precision=\"16-mixed\")\n",
        "trainer2.fit(tab, train_tab_dl, val_tab_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Évaluation sur validation & fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "y_true=[]; y_pred_img=[]\n",
        "model_img.eval()\n",
        "with torch.no_grad():\n",
        "    for b in val_dl:\n",
        "        logits = model_img(b[\"image\"]).cpu()\n",
        "        y_true += b[\"label\"].cpu().tolist()\n",
        "        y_pred_img += logits.argmax(1).tolist()\n",
        "print(\"Image-only:\\n\", classification_report(y_true, y_pred_img, target_names=CLASSES, digits=3))\n",
        "\n",
        "# Tab-only (évaluation rapide sur df_feats déjà créé)\n",
        "tab.eval()\n",
        "with torch.no_grad():\n",
        "    probs_tab = torch.softmax(tab(torch.tensor(df_feats[feat_cols].values, dtype=torch.float32)), dim=1).cpu().numpy()\n",
        "y_pred_tab = probs_tab.argmax(1)\n",
        "print(\"Tab-only:\\n\", classification_report(df_feats[\"y\"].values, y_pred_tab, target_names=CLASSES, digits=3))\n",
        "\n",
        "# Fusion naïve: si on veut fusionner sur un sous-ensemble commun, il faudrait aligner les indices; ici démo avec tab-only.\n",
        "print(\"(Astuce) Pour une vraie fusion sur val: collecter proba image et proba tab sur le même split et combiner avec fuse_probs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Inférence sur test : JSON champs + CSV classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c23db598",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- À ajouter AVANT la boucle (une seule fois) ---\n",
        "from paddleocr import PaddleOCR\n",
        "ocr_engine = PaddleOCR(use_angle_cls=True, lang='multilang', show_log=False)\n",
        "\n",
        "# Sécurise le modèle tabulaire : s'il n'existe pas, on fait image-only\n",
        "if 'tab' in globals():\n",
        "    tab.eval()\n",
        "else:\n",
        "    tab = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45e92c82",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- À COLLER EN REMPLACEMENT de ton bloc “OCR + JSON champs … -> fin” ---\n",
        "\n",
        "sub_rows = []\n",
        "model_img.eval()\n",
        "\n",
        "for imgp in test_images:\n",
        "    image_id = os.path.basename(imgp)\n",
        "    country = infer_country_from_path(imgp)\n",
        "\n",
        "    # lecture + alignement (si tu as un template)\n",
        "    img_bgr = cv2.imread(imgp)\n",
        "    if img_bgr is None:\n",
        "        print(f\"[WARN] Image illisible: {imgp}\"); \n",
        "        continue\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    template = None\n",
        "    if country in schema.templates and schema.templates[country].get(\"template_image\") is not None:\n",
        "        warped_rgb, _ = warp_to_template(img_rgb, schema.templates[country][\"template_image\"])\n",
        "    else:\n",
        "        warped_rgb = img_rgb\n",
        "\n",
        "    # ---------- OCR + JSON plat (mêmes clés que gt) ----------\n",
        "    # PaddleOCR attend du BGR → on reconvertit\n",
        "    ocr_res = ocr_engine.ocr(cv2.cvtColor(warped_rgb, cv2.COLOR_RGB2BGR), cls=True)\n",
        "    ocr_items = []\n",
        "    if ocr_res and len(ocr_res) > 0 and ocr_res[0] is not None:\n",
        "        for line in ocr_res[0]:\n",
        "            box = line[0]\n",
        "            text = line[1][0]\n",
        "            conf = float(line[1][1])\n",
        "            ocr_items.append({\"box\": box, \"text\": text, \"conf\": conf})\n",
        "\n",
        "    expected = schema.fields_for_country(country)  # [{\"key\": ...}, ...]\n",
        "    flat_fields = {}\n",
        "    for f in expected:\n",
        "        k = f[\"key\"]\n",
        "        flat_fields[k] = pick_text_for_key_from_ocr(k, ocr_items)\n",
        "\n",
        "    # Sauvegarde JSON plat (clé -> valeur), pas de bbox\n",
        "    save_json(\n",
        "        flat_fields, \n",
        "        os.path.join(SUB_DIR, \"fields_json\", image_id.rsplit(\".\", 1)[0] + \".json\")\n",
        "    )\n",
        "\n",
        "    # ---------- Probas image ----------\n",
        "    ti = t_infer(image=warped_rgb)[\"image\"].unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        p_img = torch.softmax(model_img(ti), dim=1).cpu().numpy()[0]\n",
        "\n",
        "    # ---------- Probas tab (facultatif) ----------\n",
        "    if tab is not None:\n",
        "        # features texte simples alignées avec l'entraînement tabulaire\n",
        "        confs = [it.get(\"conf\", 0.0) for it in ocr_items]\n",
        "        text_len = [min(len(it.get(\"text\",\"\")), 64) for it in ocr_items]\n",
        "        feats = {\n",
        "            \"conf_mean\": float(np.mean(confs)) if confs else 0.0,\n",
        "            \"conf_max\": float(np.max(confs)) if confs else 0.0,\n",
        "            \"ocr_lines\": float(len(ocr_items)),\n",
        "            \"text_len_mean\": float(np.mean(text_len)) if text_len else 0.0,\n",
        "        }\n",
        "        # Si feat_cols n'existe pas, on le définit ici dans le même ordre\n",
        "        if 'feat_cols' not in globals():\n",
        "            feat_cols = [\"conf_mean\", \"conf_max\", \"ocr_lines\", \"text_len_mean\"]\n",
        "\n",
        "        feat_vec = np.array([[feats.get(c, 0.0) for c in feat_cols]], dtype=np.float32)\n",
        "        with torch.no_grad():\n",
        "            p_tab = torch.softmax(tab(torch.tensor(feat_vec)), dim=1).cpu().numpy()[0]\n",
        "\n",
        "        # Fusion tardive\n",
        "        alpha = 0.6\n",
        "        p = alpha * p_img + (1.0 - alpha) * p_tab\n",
        "    else:\n",
        "        p = p_img\n",
        "\n",
        "    pred = int(np.argmax(p))\n",
        "    sub_rows.append({\"image_id\": image_id, \"class_pred\": CLASSES[pred]})\n",
        "\n",
        "# ---------- CSV final ----------\n",
        "pd.DataFrame(sub_rows).to_csv(os.path.join(SUB_DIR, \"submission.csv\"), index=False)\n",
        "print(\"Fichiers écrits dans:\", SUB_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import glob\n",
        "# from albumentations import Compose, LongestMaxSize, PadIfNeeded, Normalize\n",
        "# from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# t_infer = Compose([\n",
        "#     LongestMaxSize(768),\n",
        "#     PadIfNeeded(768,768, border_mode=cv2.BORDER_CONSTANT, value=(255,255,255)),\n",
        "#     Normalize(),\n",
        "#     ToTensorV2()\n",
        "# ])\n",
        "\n",
        "# sub_rows=[]\n",
        "# model_img.eval(); tab.eval()\n",
        "\n",
        "# test_images = []\n",
        "# for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\"):\n",
        "#     test_images += glob.glob(os.path.join(TEST_ROOT, \"**\", ext), recursive=True)\n",
        "# print(f\"Total images test: {len(test_images)}\")\n",
        "\n",
        "# for imgp in test_images:\n",
        "#     image_id=os.path.basename(imgp)\n",
        "#     country = infer_country_from_path(imgp)\n",
        "#     img=cv2.cvtColor(cv2.imread(imgp), cv2.COLOR_BGR2RGB)\n",
        "#     template=None\n",
        "#     if country in schema.templates and schema.templates[country][\"template_image\"] is not None:\n",
        "#         warped,_=warp_to_template(img, schema.templates[country][\"template_image\"])\n",
        "#     else:\n",
        "#         warped=img\n",
        "\n",
        "#     # OCR + JSON champs (structure minimale: {key: {text, bbox}})\n",
        "#     ocr_items=OCRWrapper(country).run(warped)\n",
        "#     expected = schema.fields_for_country(country)\n",
        "#     fields={}\n",
        "#     for f in expected:\n",
        "#         fields[f[\"key\"]] = {\"text\":\"\", \"bbox\": f[\"bbox\"]}\n",
        "#     # Simple appariement: nearest center (démo)\n",
        "#     for f in expected:\n",
        "#         ex=(f[\"bbox\"][0]+f[\"bbox\"][2]/2, f[\"bbox\"][1]+f[\"bbox\"][3]/2)\n",
        "#         best=None; best_d=1e18\n",
        "#         for it in ocr_items:\n",
        "#             xs=[p[0] for p in it[\"box\"]]; ys=[p[1] for p in it[\"box\"]]\n",
        "#             cx,cy=sum(xs)/4,sum(ys)/4\n",
        "#             d=(cx-ex[0])**2+(cy-ex[1])**2\n",
        "#             if d<best_d: best=it; best_d=d\n",
        "#         if best is not None:\n",
        "#             fields[f[\"key\"]][\"text\"] = best[\"text\"]\n",
        "#     save_json(fields, os.path.join(SUB_DIR, \"fields_json\", image_id.replace('.jpg','.json').replace('.png','.json')))\n",
        "\n",
        "#     # Probas image\n",
        "#     ti=t_infer(image=warped)[\"image\"].unsqueeze(0)\n",
        "#     with torch.no_grad():\n",
        "#         p_img=torch.softmax(model_img(ti), dim=1).cpu().numpy()[0]\n",
        "\n",
        "#     # Probas tab\n",
        "#     feats=build_features(ocr_items, expected)\n",
        "#     feat_vec=np.array([[feats.get(c,0.0) for c in feat_cols]], dtype=np.float32)\n",
        "#     with torch.no_grad():\n",
        "#         p_tab=torch.softmax(tab(torch.tensor(feat_vec)), dim=1).cpu().numpy()[0]\n",
        "\n",
        "#     p=fuse_probs(p_img, p_tab, alpha=0.6)\n",
        "#     pred=int(np.argmax(p))\n",
        "#     sub_rows.append({\"image_id\":image_id, \"class_pred\": CLASSES[pred]})\n",
        "\n",
        "# pd.DataFrame(sub_rows).to_csv(os.path.join(SUB_DIR, \"submission.csv\"), index=False)\n",
        "# print(\"Fichiers écrits dans:\", SUB_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Sauvegarde des poids au format `.pt`\n",
        "Lightning génère des `.ckpt`. On exporte aussi en `.pt` si besoin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model_img.state_dict(), os.path.join(WEIGHTS_DIR, \"best_img.pt\"))\n",
        "torch.save(tab.state_dict(), os.path.join(WEIGHTS_DIR, \"best_tab.pt\"))\n",
        "print(\"Poids sauvegardés dans:\", WEIGHTS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Notes\n",
        "- Adapter `DATA_ROOT` vers votre dossier de données.\n",
        "- Fournir un `template_image` par pays (facultatif mais recommandé) en modifiant `schema.set_template_image(country, img)`. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
